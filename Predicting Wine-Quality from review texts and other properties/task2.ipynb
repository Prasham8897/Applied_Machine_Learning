{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, TfidfTransformer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from category_encoders import TargetEncoder\n",
    "from sklearn.pipeline import Pipeline,make_pipeline\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.preprocessing import StandardScaler,Normalizer\n",
    "\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('winemag-data-130k-v2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop(columns = ['Unnamed: 0'])\n",
    "data = data.loc[data['country'] == 'US']\n",
    "data = data.drop(columns = ['designation', 'country','taster_name', 'taster_twitter_handle', 'title']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "description       0\n",
       "points            0\n",
       "price           239\n",
       "province          0\n",
       "region_1        278\n",
       "region_2       3993\n",
       "variety           0\n",
       "winery            0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of missing values in each column\n",
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.dropna()\n",
    "y = data[['points']]\n",
    "X = data.drop(columns = 'points')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_lg\",disable=[\"tagger\", \"parser\", \"ner\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "description_train = list(X_train[\"description\"])\n",
    "description_test = list(X_test[\"description\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_train = [nlp(d).vector for d in description_train]\n",
    "docs_test = [nlp(d).vector for d in description_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_desc = (np.vstack(docs_train))\n",
    "X_test_desc = (np.vstack(docs_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_wo_desc = X_train.drop(columns = [\"description\"])\n",
    "X_test_wo_desc = X_test.drop(columns = [\"description\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "Results for regression using Word Embedding only\n",
      "--------------------------------------------------------------------------------\n",
      "The best parameters after grid search: {'regressor__alpha': 0.29763514416313175}\n",
      "The score on test set after grid search is: 0.5631487389020682\n"
     ]
    }
   ],
   "source": [
    "pipeline_new = Pipeline([('regressor', Ridge())])\n",
    "param_grid = [{'regressor__alpha': np.logspace(-2,2,20)}]\n",
    "grid = GridSearchCV(pipeline_new, param_grid, cv = 5)\n",
    "grid.fit(X_train_desc, y_train)\n",
    "print(\"--------------------------------------------------------------------------------\")\n",
    "print(\"Results for regression using Word Embedding only\")\n",
    "print(\"--------------------------------------------------------------------------------\")\n",
    "print(\"The best parameters after grid search: \" + str(grid.best_params_))\n",
    "print(\"The score on test set after grid search is: \" + str(grid.score(X_test_desc, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we used only the word embeddings, the R2 decreased from 0.70 (using bag of words model) to 0.56.\n",
    "Next, we try combining the BoW model with the word embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_BOW_embed = pd.DataFrame(X_train_desc)\n",
    "X_train_BOW_embed[\"descriptions\"] = description_train\n",
    "X_test_BOW_embed = pd.DataFrame(X_test_desc)\n",
    "X_test_BOW_embed[\"descriptions\"] = description_test\n",
    "\n",
    "X_train_BOW_embed.columns = [str(i) for i in X_train_BOW_embed.columns]\n",
    "X_test_BOW_embed.columns = [str(i) for i in X_test_BOW_embed.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "Results for regression using Word Embedding and Bag Of Words:\n",
      "--------------------------------------------------------------------------------\n",
      "Score on the test set is: 0.6773992066579624\n"
     ]
    }
   ],
   "source": [
    "text_pipeline = make_pipeline(CountVectorizer())\n",
    "\n",
    "preprocess = make_column_transformer((text_pipeline, 'descriptions'),\n",
    "                                     remainder = \"passthrough\")\n",
    "\n",
    "pipe = make_pipeline(preprocess, Ridge(alpha=0.3))\n",
    "pipe.fit(X_train_BOW_embed, y_train)\n",
    "s1 = pipe.score(X_test_BOW_embed,y_test)\n",
    "\n",
    "print(\"--------------------------------------------------------------------------------\")\n",
    "print(\"Results for regression using Word Embedding and Bag Of Words:\")\n",
    "print(\"--------------------------------------------------------------------------------\")\n",
    "print(\"Score on the test set is: \" +str(s1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "Results for regression using Word Embedding and Bag Of Words:\n",
      "--------------------------------------------------------------------------------\n",
      "Score on the test set is: 0.7012543636242331\n"
     ]
    }
   ],
   "source": [
    "text_pipeline = make_pipeline(CountVectorizer(ngram_range=(1, 3), min_df=4, stop_words=\"english\", analyzer=\"char_wb\"), Normalizer(),TfidfTransformer())\n",
    "\n",
    "preprocess = make_column_transformer((text_pipeline, 'descriptions'),\n",
    "                                     remainder = \"passthrough\")\n",
    "\n",
    "pipe = make_pipeline(preprocess, Ridge(alpha=0.3))\n",
    "pipe.fit(X_train_BOW_embed, y_train)\n",
    "s1 = pipe.score(X_test_BOW_embed,y_test)\n",
    "\n",
    "print(\"--------------------------------------------------------------------------------\")\n",
    "print(\"Results for regression using Word Embedding and Bag Of Words:\")\n",
    "print(\"--------------------------------------------------------------------------------\")\n",
    "print(\"Score on the test set is: \" +str(s1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess = make_column_transformer((StandardScaler(), ['price']), \n",
    "                                     (TargetEncoder(), ['province','region_1','region_2','variety','winery']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_wo_desc = preprocess.fit_transform(X_train_wo_desc, y_train)\n",
    "X_test_wo_desc = preprocess.transform(X_test_wo_desc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_w_desc = np.hstack((X_train_desc,X_train_wo_desc))\n",
    "X_test_w_desc = np.hstack((X_test_desc,X_test_wo_desc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "Results for regression using Word Embedding with non-text features\n",
      "--------------------------------------------------------------------------------\n",
      "The score on test set is: 0.6734143946822839\n"
     ]
    }
   ],
   "source": [
    "r = Ridge().fit(X_train_w_desc, y_train)\n",
    "s1 = r.score(X_test_w_desc, y_test)\n",
    "\n",
    "print(\"--------------------------------------------------------------------------------\")\n",
    "print(\"Results for regression using Word Embedding with non-text features\")\n",
    "print(\"--------------------------------------------------------------------------------\")\n",
    "# print(\"The best parameters after grid search: \" + str(grid.best_params_))\n",
    "print(\"The score on test set is: \" + str(s1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After combining all the models, we get the following results - \n",
    "1. Word Embeddings alone = 0.56\n",
    "2. BoW alone = 0.67\n",
    "3. Tuned BoW alone = 0.700\n",
    "4. BoW Basic model (using only CountVectorizer) + Word Embeddings = 0.67\n",
    "5. Tuned BoW model (from 1.3) + Word Embeddings = 0.701\n",
    "6. Non-text features + Word Embeddings = 0.67\n",
    "7. Non-text features + Tuned BoW model = 0.78\n",
    "\n",
    "So, we can see that the best model is using Non-text features + Tuned BoW model.\n",
    "Adding the word embeddings to our tuned BoW model, does increase the R2 to 0.001."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
